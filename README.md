# Multiplex immunophenotyping classification 
# Point-cloud-saliency-map-based-deep-neural-network

Top: Representative mQIF of the tumor microenvironment of a breast cancer TMA core, stained for cell nuclei (DAPI, blue); CD8 (FITC, green); PDL-1 (Cy5, red); CD68 (TRITC, carmine); pan-cytokeratin (Cy7, cyan). Points (coordinate map) extracted using a commercial software package HALO where x and y-coordinates represents the physical cell location and z-coordinate represents the immunophenotype. Point cloud data used for the deep learning analysis in tiled-based processing. Bottom (1,2): The point cloud saliency map based deep learning strategy takes N points from the mQIF TMAs slides per tile. The data passes through feature extraction layers, a max pooling layer, classification layers and saliency maps construction. The feature extraction is comprised of five dense layers with 64, 64, 64, 128, and 1024 neurons, respectively, that share weights across each point. The max pooling layer applies a symmetric function reducing the features to 1024 × 1 dimensions. The output of the max pooling layer is fed to two fully connected layers with 512 and 256 neurons, respectively, which is fed to a dropout layer and finally a softmax layer for classification. Saliency maps are then constructed to detect the key phenotypes of the DNN model by calculating gradients, which guide point-dropping in an iterative process as follows: compute the gradient, compute the spherical core, compute distance to the spherical core, construct the saliency score map, dropping points.

<img width="672" alt="image" src="https://github.com/user-attachments/assets/4a06a653-7895-4e25-a087-1ff15a9edb54" />


References:
1.	Charles R. Qi, H. S., Kaichun Mo, Leonidas J. Guibas. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 652-660 (2017).
2.	Tianhang Zheng, C. C., Junsong Yuan, Bo Li, Kui Ren. PointCloud Saliency Maps. Proceedings of the IEEE/CVF International Conference on Computer Vision, 1598-1606 (2019).


# Race-associated breast tumor differences in single cell multiplex immunophenotyping 

Abstract

Breast cancer is the most commonly diagnosed cancer. Breast cancer is the leading cause of cancer death in women where non-hispance Black (NHB) patients continue to have 41% higher death rates compared to non-hispanic White (NHW) patients. It was shown that this discrepancy in clinical outcome persist even after adjustment socioeconomic factors, suggesting differences in tumor biology by race. Here, we provide clinical data analysis using point cloud saliency map based deep neural network (DNN) strategy. We classify and characterize race-associated differences from single cell multiplex quantitative immune-fluorescence tissue microarrays from unique cohort of race diverse 555 NHB and NHW breast cancer patients. The DNN model achieved significant overall validation accuracy of 0.903 (95% CI 0.907-0.899) as well as accuracy of 0.905 (95% CI 0.912-0.899) and 0.901 (95% CI 0.909-0.893) for NHB and NHW breast cancers respectively. The key differential phenotypes of the DNN were characterized into understandable descriptions of the patterns that enabled successful predictions. We show the complex architectural interplay between tumor cells and surrounding immune and stromal components that makes the regulatory distinction between NHB and NHW breast tumors. Our results indicates that NHB tumors may have different and more complex breast tumor microenvironment milieus than NHW tumors. These findings suggest that the disparity in the clinical outcome of NHB breast cancer patients is race-associated with tumor biology and that knowledge likely to improve personalized oncology and targeted therapy.


![image](https://github.com/user-attachments/assets/533f82d4-c7ff-4cfb-a9b4-957e5ac921d9)

Classification performance of DNN for single cell multiplex immunophenotyping with point clouds retrieved from 555 NHB and NHW breast tumors. A. Performance of validation accuracy for different tile sizes. B. Performance of validation for the selected tile size. C. Training and validation progress curves for every 300 epochs.






Method

Here, point clouds data structure consisted of a set of points in an artificial 3D space. Each point is represented with x and y coordinates for the physical cell location as well z coordinate (artificial) as the cell phenotype identifier. In this work, the classification network PointNet is utilized in tiled-based processing. Using any tiles of which comprising of any number of cancer cells. The network architecture consists of a section of shared dense feature extraction layers, a max pooling operation, followed by a section of densely connected layers (Fig. 1C). The network takes a point cloud comprised of N points as input. Each point i is composed of xi, yi, and zi. Before entering the feature extraction layers, the point cloud is randomly rotated around the x, y, and z axis. This transformation is required to train the network to be invariant to global rotation. Each point is identically and independently passed through the feature extraction layers. The feature extraction is a multilayer perceptron (MLP) network composed of five dense layers with 64, 64, 64, 128, and 1024 neurons, respectively. The weights of the feature extraction networks are shared across each point, similar to a convolution operation. To combine features from the different points and reach an overall classification, the network employs a symmetric function that aggregates information from each point, irrespective of initial point order. The idea is to approximate a general function by combining feature transformations and a symmetry function: f({q1, …, qn}) ≈ g(h(q1), …, h(qn)), where q1, …, qn are the input points of the point cloud, h is the pointwise network for feature extraction MLP and g, the symmetry function (max pooling function). The output of the max pooling layer is fed to two fully connected layers with 512 and 256 neurons, respectively, which is fed to a dropout layer and finally a softmax layer for classification. All fully connected layers, including the layers in the feature transformation, use batch normalization and the rectified linear unit (ReLU) nonlinear activation function. We employ loss function and use the Adam Optimizer with learning rate 0.001. The network was implemented using TensorFlow GPU.
We build a point-cloud saliency maps through the DNN models. Here we approximate point dropping by point-shifting operation (procedure of shifting points to the spherical core of the point cloud). Through this way, the nondifferentiable functional loss change caused by point-dropping. All the original external points were not droped and point cloud determine the recognition result. Consequently, dropping a point has the effects of eliminating the impact of the point on the classification result. We determine the contribution of a point by the gradient of loss under the point-dropping operation. Saliency maps are readily constructed by calculating gradients, which guide the point-dropping iterative processes as follows: compute the gradient, compute the spherical core, compute distance to the spherical core, construct the saliency score map, dropping points. Points drop is iteratively, such that point dependencies in the remaining point set will be considered when calculating saliency scores for the next iteration. Specifically, in each iteration, a new saliency map is constructed for the remaining points, and points are dropped based on the current saliency map.  



